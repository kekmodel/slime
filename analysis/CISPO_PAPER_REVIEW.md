# CISPO ë…¼ë¬¸ ë¶„ì„ ë¦¬ë·°

## ë…¼ë¬¸ ì •ë³´

**ì œëª©**: "MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention"
**ì¶œì²˜**: https://arxiv.org/html/2506.13585v1
**ì €ì**: MiniMax íŒ€
**ë¦¬ë·° ë‚ ì§œ**: 2025-10-30

## 1. ë…¼ë¬¸ ê°œìš”

MiniMax-M1ì€ Hybrid Mixture-of-Experts ì•„í‚¤í…ì²˜ì™€ Lightning Attentionì„ ê²°í•©í•œ ì˜¤í”ˆ ì›¨ì´íŠ¸ ì¶”ë¡  ëª¨ë¸ì…ë‹ˆë‹¤.

### ì£¼ìš” ìŠ¤í™
- **ì´ íŒŒë¼ë¯¸í„°**: 456B (í™œì„±í™”: 45.9B per token)
- **ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´**: 1M í† í° ì§€ì›
- **ìµœëŒ€ ìƒì„± ê¸¸ì´**: 80K í† í°
- **íš¨ìœ¨ì„±**: DeepSeek R1 ëŒ€ë¹„ 100K í† í° ìƒì„± ì‹œ 25% FLOPsë§Œ ì‚¬ìš©
- **í•™ìŠµ ë¹„ìš©**: 512x H800 GPUì—ì„œ 3ì£¼, ì•½ $534,700

## 2. CISPO ì•Œê³ ë¦¬ì¦˜ í•µì‹¬ ì›ë¦¬

### 2.0 Advantage ê³„ì‚° (GRPO ë°©ì‹)

**ë…¼ë¬¸ ìœ„ì¹˜**: Section 3.1 "Efficient RL Scaling with CISPO", Equation 2

CISPOëŠ” GRPOì˜ **group-relative advantage normalization**ì„ ì±„íƒí•©ë‹ˆë‹¤:

$$
A_{i,t} = \frac{R_i - \text{mean}(\{R_j\}_{j=1}^G)}{\text{std}(\{R_j\}_{j=1}^G)}
$$

ì—¬ê¸°ì„œ $R_i$ëŠ” ì‘ë‹µì˜ ë³´ìƒ, $G$ê°œì˜ ì‘ë‹µ $\{o_i\}_{i=1}^G$ê°€ ê·¸ë£¹ì„ êµ¬ì„±í•©ë‹ˆë‹¤.

**ì£¼ìš” íŠ¹ì§•** (ë…¼ë¬¸ Section 3.1, Equation 4):
- **Sequence-level advantage**: í•œ ì‹œí€€ìŠ¤ì˜ ëª¨ë“  í† í°ì´ ë™ì¼í•œ advantage ê³µìœ 
- **Group normalization**: ë°°ì¹˜ ë‚´ ìƒëŒ€ì  ì„±ëŠ¥ìœ¼ë¡œ ì •ê·œí™”
- **Value model ë¶ˆí•„ìš”**: ë³„ë„ì˜ baseline í•™ìŠµ ì—†ìŒ
- **ì›ë³¸ ì¶œì²˜**: Shao et al. (2024) - DeepSeekMath (GRPO ë…¼ë¬¸)

**slime êµ¬í˜„ í™•ì¸** (`loss.py:240-244`):
```python
if args.advantage_estimator in ["grpo", "gspo", "cispo"]:
    rewards = torch.tensor(rewards, dtype=torch.float32, device=kl[0].device)
    returns = get_grpo_returns(rewards, kl)  # ë³´ìƒì„ ëª¨ë“  í† í°ìœ¼ë¡œ ë¸Œë¡œë“œìºìŠ¤íŠ¸
    advantages = [r for r in returns]
```

**Whitening êµ¬í˜„** (`loss.py:294-344`, `distributed_utils.py:93-153`):
```python
if args.normalize_advantages:
    whitened_advs = distributed_masked_whiten(
        all_advs, all_masks,
        shift_mean=True,  # í‰ê·  ì œê±°
        epsilon=1e-8      # ìˆ˜ì¹˜ ì•ˆì •ì„±
    )
```

`distributed_masked_whiten`ì€ ë¶„ì‚° í™˜ê²½ì—ì„œ ê¸€ë¡œë²Œ í†µê³„ë¥¼ ì‚¬ìš©:

$$
\text{whitened}(A) = \frac{A - \mu_{\text{global}}}{\sqrt{\sigma^2_{\text{global}} + \epsilon}}
$$

**ì¤‘ìš”**: ë…¼ë¬¸ì€ í•­ìƒ ì •ê·œí™”ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, slimeì—ì„œëŠ” `--normalize-advantages` í”Œë˜ê·¸ê°€ **ê¸°ë³¸ê°’ False**ì…ë‹ˆë‹¤. CISPO í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸(`test_cispo.sh`)ì—ë„ ì´ í”Œë˜ê·¸ê°€ **ëª…ì‹œë˜ì§€ ì•ŠìŒ** â†’ í™•ì¸ í•„ìš”!

### 2.1 ë¬¸ì œ ì •ì˜

ê¸°ì¡´ PPO/GRPO/DAPOì˜ ê·¼ë³¸ì  í•œê³„:
- **í† í° ë ˆë²¨ í´ë¦¬í•‘**ìœ¼ë¡œ ì¸í•´ ì¤‘ìš”í•œ "ì¶”ë¡  í† í°"ì´ ì²« ì—…ë°ì´íŠ¸ í›„ ì‚¬ë¼ì§
- "However," "Wait," "Let me recheck..." ê°™ì€ **ë°˜ì„±ì  í† í°**ì´ ë‚®ì€ í™•ë¥ (ë†’ì€ IS ratio)ë¡œ ì¸í•´ í´ë¦¬í•‘ë¨
- Chain-of-Thought ì¶”ë¡ ì—ì„œ ì´ëŸ¬í•œ í† í°ë“¤ì´ í•µì‹¬ì ì´ì§€ë§Œ gradientê°€ ì°¨ë‹¨ë¨

### 2.2 CISPOì˜ í•´ê²°ì±…

**í•µì‹¬ ì•„ì´ë””ì–´**: í† í° ì—…ë°ì´íŠ¸ë¥¼ í´ë¦¬í•‘í•˜ëŠ” ëŒ€ì‹  **ì¤‘ìš”ë„ ìƒ˜í”Œë§ ê°€ì¤‘ì¹˜(Importance Sampling Weight)ë¥¼ í´ë¦¬í•‘**

### 2.3 ìˆ˜í•™ì  ì •ì˜

#### Importance Sampling Ratio

$$
r_{i,t}(\theta) = \frac{\pi_\theta(o_{i,t} \mid q, o_{i,<t})}{\pi_{\text{old}}(o_{i,t} \mid q, o_{i,<t})}
$$

#### CISPO ëª©ì  í•¨ìˆ˜

$$
J_{\text{CISPO}}(\theta) = \mathbb{E}\left[\frac{1}{\sum|o_i|} \sum_i \sum_t \text{sg}(\hat{r}_{i,t}(\theta)) \cdot \hat{A}_{i,t} \cdot \log \pi_\theta(o_{i,t} \mid q, o_{i,<t})\right]
$$

ì—¬ê¸°ì„œ:

$$
\hat{r}_{i,t}(\theta) = \text{clip}\left(r_{i,t}(\theta), 1 - \epsilon_{\text{low}}^{\text{IS}}, 1 + \epsilon_{\text{high}}^{\text{IS}}\right)
$$

**í•µì‹¬**: $\text{sg}(\cdot)$ëŠ” stop-gradient ì—°ì‚°ìë¡œ, **ratioì— ëŒ€í•œ gradientë¥¼ ì°¨ë‹¨**í•˜ë©´ì„œ $\log \pi_\theta$ì— ëŒ€í•œ gradientëŠ” ë³´ì¡´

### 2.4 ì•Œê³ ë¦¬ì¦˜ ë¹„êµ

| ì•Œê³ ë¦¬ì¦˜ | í´ë¦¬í•‘ ëŒ€ìƒ | Gradient íë¦„ | ì¶”ë¡  í† í° ë³´ì¡´ |
|---------|------------|--------------|---------------|
| **PPO** | í† í° ì—…ë°ì´íŠ¸ (ì–‘ë°©í–¥) | í´ë¦¬í•‘ëœ ê³± í†µê³¼ | âŒ ì²« ì—…ë°ì´íŠ¸ í›„ ì†ì‹¤ |
| **GRPO** | í† í° ì—…ë°ì´íŠ¸ (ê·¸ë£¹ ìƒëŒ€ì ) | í´ë¦¬í•‘ëœ ê³± í†µê³¼ | âŒ ì²« ì—…ë°ì´íŠ¸ í›„ ì†ì‹¤ |
| **DAPO** | í† í° ì—…ë°ì´íŠ¸ (í° ìƒí•œ) | í´ë¦¬í•‘ëœ ê³± í†µê³¼ | âš ï¸ ë¶€ë¶„ì ìœ¼ë¡œ ë³´ì¡´ |
| **CISPO** | IS ê°€ì¤‘ì¹˜ë§Œ | í´ë¦¬í•‘ ì•ˆ ëœ log_probs í†µê³¼ | âœ… ì™„ì „íˆ ë³´ì¡´ |

### 2.5 ìˆ˜ì‹ ë¹„êµ

**PPO ì†ì‹¤**:

$$
\mathcal{L}_{\text{PPO}} = -\min\left(r_{i,t}(\theta) \cdot A_{i,t}, \; \text{clamp}(r_{i,t}(\theta), 1-\epsilon, 1+\epsilon) \cdot A_{i,t}\right)
$$

> ratioì™€ advantageì˜ ê³± ìì²´ë¥¼ í´ë¦¬í•‘

**CISPO ì†ì‹¤**:

$$
\mathcal{L}_{\text{CISPO}} = -\text{sg}\left(\min(r_{i,t}(\theta), 1+\epsilon_{\text{high}})\right) \cdot A_{i,t} \cdot \log \pi_\theta(o_{i,t} \mid q, o_{i,<t})
$$

> ratioë¥¼ í´ë¦¬í•‘í•˜ê³  stop-gradient ì ìš© â†’ $\log \pi_\theta$ëŠ” ê·¸ëŒ€ë¡œ ë‚¨ì•„ì„œ gradient íë¦„

## 3. slime êµ¬í˜„ ê²€í† 

### 3.1 ì½”ë“œ ë¶„ì„ (`slime/utils/ppo_utils.py:76-123`)

```python
def compute_cispo_loss(
    ppo_kl: torch.Tensor,
    log_probs: torch.Tensor,
    advantages: torch.Tensor,
    eps_clip_high: float,
):
    # 1. IS ratio ê³„ì‚°: Ï€_current / Ï€_old
    ratio = (-ppo_kl).exp()

    # 2. ìƒí•œ í´ë¦¬í•‘ (í•˜í•œ ì—†ìŒ!)
    ratio_truncated = torch.clamp(ratio, max=eps_clip_high)

    # 3. Stop-gradient ì ìš© (CISPOì˜ í•µì‹¬!)
    ratio_sg = ratio_truncated.detach()

    # 4. CISPO ê³µì‹: sg(ratio) * advantages * log_probs
    pg_losses = -ratio_sg * advantages * log_probs

    # 5. í´ë¦¬í•‘ ë¹„ìœ¨ ì¶”ì 
    clipfrac = (ratio_truncated != ratio).float()

    return pg_losses, clipfrac
```

**í‰ê°€**: âœ… **ë…¼ë¬¸ì˜ ìˆ˜í•™ì  ì •ì˜ì™€ ì™„ë²½íˆ ì¼ì¹˜**

### 3.2 í†µí•© ì§€ì  (`slime/backends/megatron_utils/loss.py:423-424`)

```python
if args.advantage_estimator == "cispo":
    pg_loss, pg_clipfrac = compute_cispo_loss(
        ppo_kl, log_probs, advantages, args.eps_clip_high
    )
```

### 3.3 ì„¤ì • ì˜µì…˜ (`slime/utils/arguments.py:623-627`)

```python
parser.add_argument(
    "--advantage-estimator",
    type=str,
    choices=["grpo", "gspo", "cispo", "reinforce_plus_plus",
             "reinforce_plus_plus_baseline", "ppo"],
    default="grpo",
)
```

## 4. í…ŒìŠ¤íŠ¸ ì„¤ì • ê²€í†  (`tests/test_cispo.sh`)

### 4.1 CISPO ì¸ì

```bash
CISPO_ARGS=(
   --advantage-estimator cispo      # CISPO ì•Œê³ ë¦¬ì¦˜ ì„ íƒ
   --kl-loss-coef 0.00             # KL ë³´ì¡° ì†ì‹¤ ì—†ìŒ
   --kl-loss-type low_var_kl       # ë‚®ì€ ë¶„ì‚° KL ê·¼ì‚¬
   --kl-coef 0.00                  # KL reward shaping ì—†ìŒ
   --entropy-coef 0.00             # Entropy bonus ì—†ìŒ
   --eps-clip-high 5.0             # Îµ_high = 5.0 (ë…¼ë¬¸ ê¶Œì¥ê°’)
)
```

**ë¶„ì„**:
- âœ… `eps_clip_high=5.0`: ë…¼ë¬¸ ê¶Œì¥ê°’ê³¼ ë™ì¼
- âœ… ëª¨ë“  KL/entropy ê³„ìˆ˜ 0: ìˆœìˆ˜ CISPO (í•˜ì´ë¸Œë¦¬ë“œ ëª©ì  í•¨ìˆ˜ ì—†ìŒ)
- âœ… `low_var_kl`: Schulman ë¸”ë¡œê·¸ì˜ non-negative KL (ë…¼ë¬¸ì—ì„œ ì‚¬ìš©)

### 4.2 í•™ìŠµ ì„¤ì •

```bash
ROLLOUT_ARGS=(
   --num-rollout 100                # 100ë²ˆì˜ rollout
   --rollout-batch-size 8           # 8ê°œ í”„ë¡¬í”„íŠ¸/ë¼ìš´ë“œ
   --n-samples-per-prompt 4         # í”„ë¡¬í”„íŠ¸ë‹¹ 4ê°œ ì‘ë‹µ
   --rollout-max-response-len 1024  # ìµœëŒ€ 1024 í† í°
   --global-batch-size 32           # ë°°ì¹˜ í¬ê¸° 32
)
```

**ê²€ì¦**:
- ì œì•½ ì¡°ê±´ ì¶©ì¡±: $\text{rollout-batch-size} \times \text{n-samples-per-prompt} = \text{global-batch-size}$
- $8 \times 4 = 32$ âœ…

## 5. ë…¼ë¬¸ì˜ ì‹¤í—˜ ê²°ê³¼

### 5.1 ì„±ëŠ¥ (Section 4.3.1)

**AIME 2024 ë²¤ì¹˜ë§ˆí¬** (Qwen2.5-32B-base):
- DAPO ëŒ€ë¹„ **2ë°° ì†ë„ í–¥ìƒ**
- DAPOì™€ ë™ì¼í•œ ì„±ëŠ¥ì„ **50% ì ì€ ìŠ¤í…**ìœ¼ë¡œ ë‹¬ì„±

### 5.2 ì£¼ìš” ë°œê²¬

1. **ê¸´ CoTì— í•„ìˆ˜ì **: 40K-80K í† í° ì¶”ë¡  ì²´ì¸ì—ì„œ gradient ì‹ í˜¸ ë³´ì¡´
2. **ë°˜ì„±ì  í† í° ë³´ì¡´**: "However", "Recheck" ê°™ì€ ë‚®ì€ í™•ë¥  í† í°ì˜ ê¸°ì—¬ë„ ìœ ì§€
3. **í´ë¦¬í•‘ ë¹„ìœ¨ ê°ì†Œ**: PPO ëŒ€ë¹„ ì ì€ í† í°ì´ í´ë¦¬í•‘ë¨ (pg_clipfrac ë‚®ìŒ)

## 6. ë…¼ë¬¸ì˜ ì¶”ê°€ ê¸°ìˆ  ì‚¬í•­

### 6.1 ì •ë°€ë„ ìˆ˜ì • (Section 4.3.2)

**ë¬¸ì œ**: Train/inference log-prob ë¶ˆì¼ì¹˜

**í•´ê²°ì±…**: LM headë¥¼ FP32ë¡œ ìƒí–¥
```python
self.lm_head = nn.Linear(...).to(torch.float32)
```

âš ï¸ **slimeì—ì„œ í™•ì¸ í•„ìš”**: í˜„ì¬ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ì— ëª…ì‹œë˜ì§€ ì•ŠìŒ

### 6.2 AdamW í•˜ì´í¼íŒŒë¼ë¯¸í„°

ë…¼ë¬¸ì—ì„œ ì‚¬ìš©í•œ ë§ì¶¤ ì„¤ì •:
```python
optimizer = AdamW(
    params,
    betas=(0.9, 0.95),    # Î²â‚‚: 0.999 â†’ 0.95ë¡œ ê°ì†Œ
    eps=1e-15,            # Îµ: 1e-8 â†’ 1e-15ë¡œ ì¦ê°€
)
```

**ì´ìœ **: MiniMax-M1ì˜ gradient íŠ¹ì„±ì— ë§ì¶¤

- $\beta_2$: $0.999 \rightarrow 0.95$ (ë” ì‘ì€ ì§€ìˆ˜ ì´ë™ í‰ê·  ìœˆë„ìš°)
- $\epsilon$: $10^{-8} \rightarrow 10^{-15}$ (ë” ë†’ì€ ìˆ˜ì¹˜ ì•ˆì •ì„±)

### 6.3 Early Truncation

**ë°©ë²•**: ë°˜ë³µ íŒ¨í„´ ê°ì§€ë¡œ ë³‘ë¦¬ì  ì‹œí€€ìŠ¤ ì¡°ê¸° ì¢…ë£Œ

âš ï¸ **slime í™•ì¸ í•„ìš”**: ì´ ê¸°ëŠ¥ êµ¬í˜„ ì—¬ë¶€ í™•ì¸ í•„ìš”

### 6.4 Staged Window Expansion

**ê¸´ ì¶”ë¡  ìŠ¤ì¼€ì¼ë§ ì „ëµ**:
```
40K í† í° â†’ 80K í† í° ë‹¨ê³„ì  í™•ì¥
```

**í•´ê²°í•œ ë¬¸ì œ**:
- íŒ¨í„´ ë¶•ê´´ (pattern collapse)
- Negative ìƒ˜í”Œ ë¶ˆê· í˜•

## 7. ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

### 7.1 í•„ìˆ˜ ë©”íŠ¸ë¦­ (ì²« ìŠ¤í…, `TESTING_CISPO.md` ê¸°ë°˜)

```python
# rollout_id=0, step=0ì—ì„œ ë°˜ë“œì‹œ í™•ì¸
assert train/ppo_kl == 0.0        # âœ… ì´ë¯¸ ì²´í¬ ì¤‘
assert train/pg_clipfrac == 0.0   # âœ… ì´ë¯¸ ì²´í¬ ì¤‘
assert train/kl_loss == 0.0       # --use-kl-loss ì‚¬ìš© ì‹œ
```

**ì¤‘ìš”ì„±**: Recomputed log-probì´ rolloutê³¼ ì •í™•íˆ ì¼ì¹˜í•¨ì„ ì¦ëª…

### 7.2 ëª¨ë‹ˆí„°ë§ ë©”íŠ¸ë¦­

```bash
train/loss          # ê°ì†Œí•´ì•¼ í•¨
train/pg_loss       # Policy gradient ì†ì‹¤
train/ppo_kl        # KL divergence (ì²« ìŠ¤í… 0!)
train/pg_clipfrac   # í´ë¦¬í•‘ëœ ratio ë¹„ìœ¨ (CISPO íŠ¹í™”)
train/entropy_loss  # Policy entropy
```

### 7.3 CISPO íŠ¹í™” ì²´í¬

1. **Ratio Truncation**: ratio > 5.0ì¼ ë•Œ `pg_clipfrac` ì¦ê°€ í™•ì¸
2. **Stop-Gradient**: ì†ì‹¤ì´ ì—¬ì „íˆ backpropagate (grad norm í™•ì¸)
3. **Sequence-Level IS**: KLì´ í† í°ë‹¹ì´ ì•„ë‹Œ ì‹œí€€ìŠ¤ë‹¹ í‰ê· ì¸ì§€ í™•ì¸

## 8. slime êµ¬í˜„ê³¼ ë…¼ë¬¸ì˜ ì ì¬ì  ì°¨ì´ì 

### 8.1 Sequence-Level vs Token-Level IS

**ë…¼ë¬¸ ì–¸ê¸‰**:
> "We use sequence-level IS ratios averaged per sequence, not per token"

**slime êµ¬í˜„ í™•ì¸** (`loss.py:399-414`):

âœ… **CISPOëŠ” GSPOì™€ ë™ì¼í•œ sequence-level IS ê²½ë¡œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤**

```python
if args.advantage_estimator in ["gspo", "cispo"]:
    # 1. ì „ì²´ ì‹œí€€ìŠ¤ì˜ log-prob ìˆ˜ì§‘
    full_log_probs = [all_gather_with_cp(...) for ...]
    full_old_log_probs = [all_gather_with_cp(...) for ...]

    # 2. ì‹œí€€ìŠ¤ë‹¹ í‰ê·  KL ê³„ì‚° (í•µì‹¬!)
    ppo_kl = [
        ((old_logprob - log_prob) * loss_mask).sum() /
        torch.clamp_min(loss_mask.sum(), 1)
        for log_prob, old_logprob, loss_mask in ...
    ]

    # 3. ê° í† í°ìœ¼ë¡œ ë¸Œë¡œë“œìºìŠ¤íŠ¸ (ê°™ì€ ì‹œí€€ìŠ¤ì˜ ëª¨ë“  í† í°ì´ ë™ì¼í•œ ratio)
    ppo_kl = [kl.expand_as(log_prob) for kl, log_prob in ...]
```

**PPO/GRPOì™€ì˜ ì°¨ì´** (`loss.py:417-420`):
```python
else:
    # í† í°ë³„ ê°œë³„ KL (token-level IS)
    ppo_kl = old_log_probs - log_probs
```

**í•µì‹¬ ì°¨ì´**:
- **CISPO/GSPO**: í•œ ì‹œí€€ìŠ¤ì˜ ëª¨ë“  í† í°ì´ ë™ì¼í•œ IS ratio ê³µìœ  â†’ ë‚®ì€ í™•ë¥  í† í°ë„ í‰ê· ì— í¬ì„ë¨
- **PPO/GRPO**: ê° í† í°ì´ ê³ ìœ  IS ratio â†’ ë‚®ì€ í™•ë¥  í† í°ì´ ê°œë³„ì ìœ¼ë¡œ í´ë¦¬í•‘ë¨

### 8.2 í˜„ì¬ êµ¬í˜„ ìƒíƒœ

| ê¸°ëŠ¥ | ë…¼ë¬¸ | slime êµ¬í˜„ | ìƒíƒœ |
|-----|------|-----------|------|
| Upper truncation only | âœ… | âœ… | ì¼ì¹˜ |
| Stop-gradient on ratio | âœ… | âœ… | ì¼ì¹˜ |
| eps_clip_high=5.0 | âœ… | âœ… | ì¼ì¹˜ |
| **Sequence-level IS** | âœ… | âœ… | **ì¼ì¹˜** (GSPOì™€ ë™ì¼ ê²½ë¡œ) |
| **Advantage normalization** | âœ… (Z-Score) | âœ… (Dr. GRPO) | **ê°œì„ ë¨** (binary reward ìµœì í™”) |
| **FP32 LM head** | âœ… | âœ… | **ì¼ì¹˜** (í…ŒìŠ¤íŠ¸ì— ì¶”ê°€ ì™„ë£Œ) |
| Repetition detection | âœ… | â“ | í™•ì¸ í•„ìš” |
| Custom AdamW params | âœ… | â“ | í™•ì¸ í•„ìš” |

## 9. ê¶Œì¥ ì‚¬í•­

### 9.1 ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥

1. âœ… **Binary Rewardì—ëŠ” Mean-Centering ê¶Œì¥**:
   ```bash
   CISPO_ARGS=(
      --advantage-estimator cispo
      --disable-grpo-std-normalization  # â† Dr. GRPO (mean-centeringë§Œ)
      --kl-loss-coef 0.00
      --kl-loss-type low_var_kl
      --kl-coef 0.00
      --entropy-coef 0.00
      --eps-clip-high 5.0
   )
   ```
   **ì´ìœ **: GSM8KëŠ” binary reward (0/1). ë¶„ì„ ê²°ê³¼ mean-centeringì´ ë” ì•ˆì •ì ì´ê³  íš¨ìœ¨ì 

   **ìƒì„¸ ë¶„ì„**: `BINARY_REWARD_ANALYSIS.md` ì°¸ì¡°

2. âš ï¸ **ë…¼ë¬¸ì€ Z-Score ì‚¬ìš©**:
   - MiniMax-M1ì€ Z-Score normalization ëª…ì‹œ (Section 3.1, Eq. 2)
   - Binary rewardì—ì„œë„ ì‘ë™í•˜ì§€ë§Œ ì—­ì„¤ì  í–‰ë™ (ì‰¬ìš´ ë¬¸ì œì— 2.3ë°° í° gradient)
   - Dr. GRPOê°€ ì´ë¡ ì ìœ¼ë¡œ ë” í•©ë¦¬ì 

3. âœ… **ê²€ì¦ ë©”íŠ¸ë¦­ í™•ì¸**: `train/ppo_kl=0.0`, `train/pg_clipfrac=0.0` (ì²« ìŠ¤í…)

### 9.2 ì™„ë£Œëœ ê°œì„  ì‚¬í•­

1. âœ… **Dr. GRPO (Mean-Centering)**: Binary reward ìµœì í™”
   ```bash
   --disable-grpo-std-normalization
   ```
   **íš¨ê³¼**: ì•ˆì •ì  gradient, ìì—°ìŠ¤ëŸ¬ìš´ ë‚œì´ë„ ê°€ì¤‘, ê·¹ë‹¨ ì¼€ì´ìŠ¤ ì•ˆì •ì„±

2. âœ… **FP32 LM head**: Training/inference precision ì¼ì¹˜
   ```bash
   --sglang-enable-fp32-lm-head
   ```
   **íš¨ê³¼**: Log-prob ì¼ì¹˜ í–¥ìƒ, ìˆ˜ì¹˜ ì•ˆì •ì„± (MiniMax-M1 Section 4.3.2)

### 9.3 í–¥í›„ ê°œì„  ê°€ëŠ¥ ì‚¬í•­

1. **AdamW í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹** (ë…¼ë¬¸ ê¶Œì¥):
   ```bash
   OPTIMIZER_ARGS=(
      --optimizer adam
      --adam-beta1 0.9
      --adam-beta2 0.95   # 0.98 â†’ 0.95 (ë…¼ë¬¸ ì‚¬ìš©)
      --adam-eps 1e-15    # ì¶”ê°€ (ë…¼ë¬¸ ì‚¬ìš©)
   )
   ```

2. **Early truncation êµ¬í˜„**:
   - ë°˜ë³µ íŒ¨í„´ ê°ì§€ ë¡œì§ ì¶”ê°€
   - ë³‘ë¦¬ì  ì‹œí€€ìŠ¤ ì¡°ê¸° ì¢…ë£Œ

3. **ê¸´ ì‹œí€€ìŠ¤ í™•ì¥** (í˜„ì¬ 1024 í† í° â†’ ë” ê¸¸ê²Œ):
   ```bash
   # ë‹¨ê³„ì  í™•ì¥ ì „ëµ (ë…¼ë¬¸: 40K â†’ 80K)
   --rollout-max-response-len 2048  # ë˜ëŠ” 4096, 8192...
   ```

### 9.4 GSM8K í…ŒìŠ¤íŠ¸ìš©

í˜„ì¬ ì„¤ì • (`--rollout-max-response-len 1024`)ì€ GSM8Kì— ì¶©ë¶„:
- ìˆ˜í•™ ë¬¸ì œëŠ” ë³´í†µ ì§§ì€ CoT (< 1K í† í°)
- ì¥ê¸° ì¶”ë¡  (40K-80K) í…ŒìŠ¤íŠ¸ëŠ” ë‹¤ë¥¸ ë²¤ì¹˜ë§ˆí¬ í•„ìš”

## 10. ê²°ë¡ 

### 10.1 êµ¬í˜„ ìƒíƒœ

âœ… **slimeì˜ CISPO êµ¬í˜„ì€ ë…¼ë¬¸ê³¼ ìˆ˜í•™ì ìœ¼ë¡œ ì¼ì¹˜**
âœ… **Sequence-level IS í™•ì¸ ì™„ë£Œ** (GSPOì™€ ë™ì¼í•œ ê²½ë¡œ, `loss.py:399-414`)
âœ… **Stop-gradient ë° upper truncation ì •í™•íˆ êµ¬í˜„ë¨** (`ppo_utils.py:76-123`)
âœ… **í…ŒìŠ¤íŠ¸ ì„¤ì •ì´ ì ì ˆí•¨** (`eps_clip_high=5.0`)
âœ… **Dr. GRPO (Mean-Centering) ì ìš©** (binary reward ìµœì í™”, `--disable-grpo-std-normalization`)
âœ… **FP32 LM head ì ìš©** (ìˆ˜ì¹˜ ì•ˆì •ì„±, `--sglang-enable-fp32-lm-head`)
âœ… **í”„ë¡œë•ì…˜ ì¤€ë¹„ ì™„ë£Œ** (MiniMaxê°€ 456B ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©)

### 10.2 ê°œì„  ì‚¬í•­

ğŸ¯ **Binary Reward ìµœì í™”**: Dr. GRPOë¡œ ì•ˆì •ì ì´ê³  íš¨ìœ¨ì ì¸ í•™ìŠµ
ğŸ¯ **ì •ë°€ë„ ì¼ì¹˜**: FP32 LM headë¡œ training/inference log-prob ì¼ì¹˜
ğŸ¯ **ì´ë¡ ì  ìš°ìœ„**: ë…¼ë¬¸ Z-Scoreë³´ë‹¤ mean-centeringì´ ë” í•©ë¦¬ì  (ê·¹ë‹¨ ì¼€ì´ìŠ¤ ì•ˆì •ì„±)

### 10.3 í–¥í›„ ê³ ë ¤ì‚¬í•­

âš ï¸ **AdamW í•˜ì´í¼íŒŒë¼ë¯¸í„°** íŠœë‹ ê°€ëŠ¥ì„± (ë…¼ë¬¸: $\beta_2=0.95$, $\epsilon=10^{-15}$)
âš ï¸ **Early truncation** êµ¬í˜„ ê³ ë ¤ (ë°˜ë³µ íŒ¨í„´ ê°ì§€)
âš ï¸ **ê¸´ ì‹œí€€ìŠ¤ í™•ì¥** í…ŒìŠ¤íŠ¸ (í˜„ì¬ 1024 â†’ ë…¼ë¬¸ 40K-80K)

### 10.4 CISPOì˜ ì¥ì 

1. **íš¨ìœ¨ì„±**: DAPO ëŒ€ë¹„ 2ë°° ë¹ ë¦„, 50% ì ì€ ìŠ¤í…
2. **ì¶”ë¡  í’ˆì§ˆ**: ë°˜ì„±ì  í† í° ë³´ì¡´ìœ¼ë¡œ CoT ê°œì„ 
3. **í™•ì¥ì„±**: 80K í† í°ê¹Œì§€ í…ŒìŠ¤íŠ¸ë¨
4. **ë‹¨ìˆœì„±**: PPOë³´ë‹¤ êµ¬í˜„ì´ ê°„ë‹¨ (value model ë¶ˆí•„ìš”)

### 10.5 ë‹¤ìŒ ë‹¨ê³„

1. **í˜„ì¬ í…ŒìŠ¤íŠ¸ ì‹¤í–‰**: `bash tests/test_cispo.sh`
2. **ì²« ìŠ¤í… ê²€ì¦**: KL=0, clipfrac=0 í™•ì¸
3. **WandB ê²°ê³¼ ìˆ˜ì§‘**: ë©”íŠ¸ë¦­ ì¶”ì  ë° ì‹œê°í™”
4. **ê¸´ ì‹¤í–‰ í…ŒìŠ¤íŠ¸**: `--num-rollout 100` ì™„ë£Œ í›„ ë¶„ì„
5. **PR ì—…ë°ì´íŠ¸**: ê²°ê³¼ ë° ë…¼ë¬¸ ê·¼ê±° ì¶”ê°€

## ì°¸ê³  ìë£Œ

- **MiniMax-M1 ë…¼ë¬¸**: https://arxiv.org/html/2506.13585v1
- **Dr. GRPO ë…¼ë¬¸**: https://arxiv.org/pdf/2503.20783
- **Schulman KL ê·¼ì‚¬**: http://joschu.net/blog/kl-approx.html
- **slime êµ¬í˜„**: `slime/utils/ppo_utils.py:76-123`, `slime/ray/rollout.py:176-181`
- **Binary Reward ë¶„ì„**: `BINARY_REWARD_ANALYSIS.md`
- **í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸**: `tests/test_cispo.sh`

---

**ë¬¸ì„œ ì‘ì„±ì¼**: 2025-10-30
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-11-01 (Sequence-level IS êµ¬í˜„ í™•ì¸ ì™„ë£Œ)
**ë¦¬ë·°ì–´**: Claude Code
**ë…¼ë¬¸ ë²„ì „**: v1 (2025ë…„ 6ì›”)
**slime ë¸Œëœì¹˜**: dev
