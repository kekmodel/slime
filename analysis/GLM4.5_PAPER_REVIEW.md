# GLM-4.5 ë…¼ë¬¸ ë¶„ì„ ë¦¬ë·°

## ë…¼ë¬¸ ì •ë³´

**ì œëª©**: "GLM-4.5: Balancing Performance and Efficiency with 300B Activated MoE"
**ì¶œì²˜**: https://arxiv.org/html/2508.06471
**ì €ì**: GLM Team (Zhipu AI)
**ë¦¬ë·° ë‚ ì§œ**: 2025-11-01

---

## 1. ë…¼ë¬¸ ê°œìš”

GLM-4.5ëŠ” Mixture-of-Experts (MoE) ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ë¡œ, **slime í”„ë ˆì„ì›Œí¬**ë¥¼ í™œìš©í•˜ì—¬ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤.

### ì£¼ìš” ìŠ¤í™

| ëª¨ë¸ | ì´ íŒŒë¼ë¯¸í„° | í™œì„±í™” íŒŒë¼ë¯¸í„° | ì»¨í…ìŠ¤íŠ¸ | íŠ¹ì§• |
|------|------------|----------------|----------|------|
| **GLM-4.5** | 355B | 32B per token | 128K | 89 MoE + 3 dense layers |
| **GLM-4.5-Air** | 106B | 12B per token | 128K | 45 MoE + 1 dense layer |

### ì„±ëŠ¥ í•˜ì´ë¼ì´íŠ¸

**Reasoning**:
- AIME 24: 91.0%
- MATH-500: 98.2%
- GPQA: 79.1%

**Coding**:
- SWE-bench Verified: 64.2%
- Terminal-Bench: 37.5%

**Agentic**:
- TAU-Bench average: 58.1%
- BFCL V3: 77.8%

**íš¨ìœ¨ì„±**:
- DeepSeek-R1 (671B) ëŒ€ë¹„ **ì ˆë°˜ì˜ íŒŒë¼ë¯¸í„°**ë¡œ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥
- Pareto frontierì— ìœ„ì¹˜ (SWE-bench vs parameters)

---

## 2. Training Pipeline

### 2.1 Pre-Training (23T tokens)

**Data Composition**:
1. **Webpages**: í’ˆì§ˆ bucketing + SemDedup íŒŒì´í”„ë¼ì¸
2. **Code**: 3-tier í’ˆì§ˆ í•„í„°ë§ GitHub ì†ŒìŠ¤
3. **Math/Science**: ì›¹í˜ì´ì§€, ì±…, ë…¼ë¬¸
4. **Multilingual**: í’ˆì§ˆ ë¶„ë¥˜

**Two-Stage Process**:
```
Stage 1: General documents @ 4K sequence length
Stage 2: Code/Math/Science upsampling @ extended lengths
```

**Optimizer**: **Muon optimizer** (Newton-Schulz iterations=5, momentum=0.95)

### 2.2 Mid-Training

1. **Repo-level Code**: Cross-file dependencies @ 32K
2. **Synthetic Reasoning**: Math/science/coding competition synthesis
3. **Long-context & Agent**: Extended to 128K with synthetic trajectories

### 2.3 Post-Training (RL + SFT)

**Multi-stage Approach**:
1. Supervised Fine-Tuning (SFT)
2. Reinforcement Learning (RL)
3. Iterative distillation (RL â†’ SFT â†’ RL)

---

## 3. RL Methodology (í•µì‹¬!)

### 3.1 RL Algorithm: GRPO

**ë…¼ë¬¸ ëª…ì‹œ**:
> "We build upon the GRPO framework, excluding the KL loss term."

**Loss Function**:

$$
\mathcal{L}_{RL}(\theta) = \mathbb{E}_{x \sim \mathcal{D}}\left[\frac{1}{K}\sum_{i=1}^K (r(x, y_i) - \bar{r}(x))\right]
$$

ì—¬ê¸°ì„œ:
- $r(x, y_i)$: ì‘ë‹µ $y_i$ì˜ reward
- $\bar{r}(x)$: ê°™ì€ prompt $x$ì—ì„œ ìƒì„±ëœ Kê°œ ì‘ë‹µì˜ í‰ê·  reward
- **KL loss term ì—†ìŒ**

**ì¤‘ìš”**: **Mean-centeringë§Œ ì‚¬ìš©** (í‘œì¤€í¸ì°¨ë¡œ ë‚˜ëˆ„ì§€ ì•ŠìŒ) â†’ **Dr. GRPOì™€ ë™ì¼!**

### 3.2 Advantage Normalization ë°©ë²•

GLM-4.5ëŠ” ëª…ì‹œì ìœ¼ë¡œ **mean-centeringë§Œ ì‚¬ìš©**:

$$
A_i = r(x, y_i) - \bar{r}(x)
$$

**Z-Score (ì‚¬ìš© ì•ˆ í•¨)**:

$$
A_i \neq \frac{r(x, y_i) - \bar{r}(x)}{\sigma_r + \epsilon}
$$

**ì´ìœ ** (`BINARY_REWARD_ANALYSIS.md` ì°¸ì¡°):
- Binary reward (0/1)ì—ì„œ Z-ScoreëŠ” ì—­ì„¤ì  í–‰ë™ (ê·¹ë‹¨ ì„±ê³µë¥ ì—ì„œ 2.3-3ë°° ì¦í­)
- Mean-centeringì€ ì•ˆì •ì ì´ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë‚œì´ë„ ê°€ì¤‘

**slime êµ¬í˜„**:
```bash
--disable-grpo-std-normalization  # Dr. GRPO (mean-centering only)
```

### 3.3 Reasoning RL Innovations

#### 3.3.1 Difficulty-based Curriculum Learning

**Two-Stage Approach**:
```
Stage 1: Moderate difficulty problems
Stage 2: Extremely difficult problems (verified correct answers only)
```

**ë¬¸ì œ í•´ê²°**:
- Reward variance ì´ìŠˆ (all 0s or all 1s)
- ì ì§„ì  ë‚œì´ë„ ì¦ê°€

#### 3.3.2 Single-Stage RL @ 64K Output Length

**ë°œê²¬**: ì ì§„ì  ê¸¸ì´ í™•ì¥ë³´ë‹¤ **ì§ì ‘ ìµœëŒ€ ê¸¸ì´ í•™ìŠµ**ì´ ìš°ìˆ˜

**ì´ìœ **:
- ê¸´ ì»¨í…ìŠ¤íŠ¸ ëŠ¥ë ¥ì˜ "unlearning" ë°©ì§€
- ë‹¨ì¼ stageì—ì„œ íš¨ìœ¨ì  í•™ìŠµ

#### 3.3.3 Dynamic Sampling Temperature

**Mechanism**:
1. Reward plateau ê°ì§€ â†’ ìˆ˜ë ´ ë‹¨ê³„ ì¸ì‹
2. Sampling temperature ì¦ê°€ë¡œ exploration ìœ ì§€
3. í’ˆì§ˆ ê²€ì¦: ì„±ëŠ¥ ì €í•˜ <1%ì¸ ìµœëŒ€ temperature ì‚¬ìš©

**slime ì§€ì›**: âŒ (í–¥í›„ êµ¬í˜„ ê°€ëŠ¥)

**êµ¬í˜„ ì•„ì´ë””ì–´**:
```python
if avg_reward.std() < threshold:  # plateau ê°ì§€
    temperature = find_max_temperature(
        constraint=lambda t: performance_drop(t) < 0.01
    )
```

#### 3.3.4 Code RL: Token-weighted Loss

**ê³µì‹**:
- **Sequence-mean loss** (ê¸°ì¡´): $\frac{1}{T}\sum_{t=1}^T \mathcal{L}_t$
- **Token-weighted mean** (GLM-4.5): $\frac{\sum_{t=1}^T w_t \mathcal{L}_t}{\sum_{t=1}^T w_t}$

**íš¨ê³¼**: ì¤‘ìš” í† í°ì— ë” í° ê°€ì¤‘ì¹˜ ë¶€ì—¬

**slime êµ¬í˜„**: âš ï¸ í™•ì¸ í•„ìš” (ê¸°ë³¸ê°’: sequence-mean)

### 3.4 Agentic RL

#### 3.4.1 Reward Design

**Task-specific Rewards**:
- **Web search**: Final answer accuracy
- **Coding (SWE)**: Verifiable test cases
- **Process format penalty**: Incorrect tool calls

#### 3.4.2 Outcome Supervision

**í•µì‹¬ ì›ì¹™**:
> "Only model-generated tokens are used for optimization, and the environment feedback is ignored in loss computation."

**êµ¬í˜„**:
```python
loss_mask[environment_tokens] = 0  # í™˜ê²½ í”¼ë“œë°± ë¬´ì‹œ
loss_mask[model_tokens] = 1        # ëª¨ë¸ ìƒì„±ë§Œ í•™ìŠµ
```

**slime ì§€ì›**: âœ… (custom_generate_functionì—ì„œ loss_mask ì„¤ì • ê°€ëŠ¥)

ì˜ˆì‹œ (`CLAUDE.md`):
```python
async def generate(args, sample: Sample, sampling_params) -> Sample:
    # Set loss_mask:
    # - 1 for model-generated tokens
    # - 0 for tool/environment outputs
    sample.loss_mask = compute_mask(sample)
    return sample
```

#### 3.4.3 Iterative Distillation

**í”„ë¡œì„¸ìŠ¤**:
```
RL-trained responses â†’ SFT data â†’ Next RL round
```

**íš¨ê³¼**: ì—°ì†ì ì¸ ê°œì„  (successive improvements)

**slime ì§€ì›**: âœ… (offline dataë¡œ ì¬í•™ìŠµ ê°€ëŠ¥)

#### 3.4.4 Test-time Scaling

**ê´€ì°°**: í™˜ê²½ interaction turns ì¦ê°€ â†’ ì„±ëŠ¥ í–¥ìƒ

**êµ¬í˜„**: Multi-turn agentic tasks with environment loops

### 3.5 General RL Components

#### 3.5.1 Holistic RL

- **~5,000 balanced prompts** across 179 categories
- **Hybrid feedback**: Human + AI annotations

#### 3.5.2 Instruction Following RL

- **7 major + 151 minor constraint taxonomy**
- ì„¸ë°€í•œ instruction following ëŠ¥ë ¥ í–¥ìƒ

#### 3.5.3 Function Calling RL

**Step-wise Rule-based Reward**:

$$
\text{Reward} = \begin{cases}
1 & \text{if FormatCorrect}(a_t) \land \text{Match}(a_t, a_t^*) \\
0 & \text{otherwise}
\end{cases}
$$

**Binary reward** (0 ë˜ëŠ” 1) â†’ **Mean-centering ê¶Œì¥**

**slime ì„¤ì •**:
```bash
--rm-type rule_based
--disable-grpo-std-normalization
```

**End-to-end Multi-turn**:
- Task completion verification
- Environment feedback loop

#### 3.5.4 Pathology RL

**Target Issues**:
- Language mixing
- Repetition
- Formatting issues

**Dataset**: Targeted pathology examples

---

## 4. Reward Model & Verifier Design

### 4.1 Multi-source Feedback System

| Feedback Type | ì‚¬ìš© ì‚¬ë¡€ | íŠ¹ì§• |
|---------------|----------|------|
| **Rule-based** | Math, Coding | Deterministic verification |
| **Human** | Subjective tasks | RLHF preference annotations |
| **Model-based** | General domain | RLAIF with scoring rubrics |

### 4.2 Binary Rewards

**ì‚¬ìš©ì²˜**:
- Function calling: Format correctness + exact match
- Math problems: Programmatic correctness
- Coding: Test case pass/fail

**ì²˜ë¦¬ ë°©ë²•**: Mean-centering (no Z-Score)

### 4.3 Verification Methods

| Task | Verification Method |
|------|-------------------|
| **Math** | Programmatic correctness checking |
| **Subjective** | Trained reward model on preferences |
| **Agentic** | Automated environment feedback or LLM Judge |

---

## 5. slime Framework ì‚¬ìš©

### 5.1 ë…¼ë¬¸ì˜ ëª…ì‹œì  ì–¸ê¸‰

> "We developed and utilized the Slime RL framework, which supports both colocated synchronous and disaggregated asynchronous modes."

**slime ì—­í• **:
1. **Reasoning/Math RL**: Colocated synchronous mode
2. **Agentic/SWE RL**: Disaggregated asynchronous mode
3. **FP8 Inference**: BF16 training + FP8 rollout quantization
4. **Docker Runtime**: Isolated task environments
5. **HTTP Interface**: Heterogeneous agent framework integration

### 5.2 Precision Strategy

**Training**: BF16
**Inference (Rollout)**: FP8 (online, block-wise quantization)

**slime ì§€ì›**:
```bash
# BF16 training (ê¸°ë³¸ê°’)
# FP8 inference
--hf-checkpoint /path/to/model-FP8
```

ì˜ˆ: `Qwen/Qwen3-4B-FP8`, `Qwen/Qwen3-30B-A3B-FP8`

### 5.3 Colocated vs Disaggregated

**Colocated Synchronous** (Reasoning RL):
```bash
--actor-num-nodes 1
--actor-num-gpus-per-node 8
--colocate
--sglang-mem-fraction-static 0.8
```

**Disaggregated Asynchronous** (Agentic RL):
```bash
--actor-num-nodes 1
--actor-num-gpus-per-node 4
--rollout-num-gpus 4
```

---

## 6. ì•„í‚¤í…ì²˜ í˜ì‹ 

### 6.1 Loss-free Balance Routing

**ê¸°ì¡´ MoE ë¬¸ì œ**: Load imbalance â†’ auxiliary loss í•„ìš”

**GLM-4.5 í•´ê²°ì±…**: Sigmoid gatesë¡œ loss-free balancing

### 6.2 QK-Norm

**ëª©ì **: Attention logit stabilization

### 6.3 MoE as Multi-Token Prediction (MTP)

**ì—­í• **: Speculative decoding layer

**íš¨ê³¼**: Inference ì†ë„ í–¥ìƒ

### 6.4 Deeper, Narrower Architecture

**ë°œê²¬**:
> "Deeper models exhibited better reasoning capacity"

**ì„¤ê³„ ì„ íƒ**:
- ë” ë§ì€ layers (92 vs fewer in GLM-4)
- ë” ì‘ì€ hidden dimension (5120)

### 6.5 Novel Function Call Template

**ê¸°ì¡´ (JSON-based)**:
```json
{"name": "function", "parameters": {"code": "<script>"}}
```
ë¬¸ì œ: Code segmentì—ì„œ character escaping ë¶€ë‹´

**GLM-4.5 (XML-like)**:
```xml
<tool_call>
<name>function</name>
<code><![CDATA[
  script here
]]></code>
</tool_call>
```

**ì¥ì **: Escaping burden ê°ì†Œ

---

## 7. slime êµ¬í˜„ê³¼ ë¹„êµ

### 7.1 êµ¬í˜„ ìƒíƒœ

| ê¸°ëŠ¥ | GLM-4.5 | slime êµ¬í˜„ | ìƒíƒœ |
|------|---------|-----------|------|
| GRPO (no KL term) | âœ… | âœ… | `--kl-coef 0.0` |
| **Mean-centering only** | âœ… | âœ… | `--disable-grpo-std-normalization` |
| Binary reward handling | âœ… | âœ… | Same formula |
| FP8 inference | âœ… | âœ… | HF FP8 models |
| BF16 training | âœ… | âœ… | Default |
| Outcome supervision | âœ… | âœ… | `loss_mask` in custom_generate |
| Colocated/Disaggregated | âœ… | âœ… | Both modes supported |
| **Dynamic temperature** | âœ… | âŒ | í–¥í›„ êµ¬í˜„ í•„ìš” |
| **Token-weighted loss** | âœ… (Code RL) | âš ï¸ | í™•ì¸ í•„ìš” |
| Iterative distillation | âœ… | âœ… | Offline data retraining |

### 7.2 GLM-4.5 ì¬í˜„ì„ ìœ„í•œ slime ì„¤ì •

#### Reasoning/Math RL (GSM8K, MATH)

```bash
ROLLOUT_ARGS=(
   --prompt-data math_dataset.parquet
   --input-key question
   --label-key answer
   --apply-chat-template
   --rollout-shuffle
   --rm-type math
   --num-rollout 100
   --rollout-batch-size 8
   --n-samples-per-prompt 4
   --rollout-max-response-len 64000  # GLM-4.5: 64K output
   --rollout-temperature 0.8
   --global-batch-size 32
)

RL_ARGS=(
   --advantage-estimator grpo
   --disable-grpo-std-normalization  # Mean-centering only (GLM-4.5 ë°©ì‹)
   --kl-loss-coef 0.00              # No KL term
   --kl-coef 0.00
   --entropy-coef 0.00
)

PRECISION_ARGS=(
   --hf-checkpoint /path/to/model-FP8  # FP8 inference
   --ref-load /path/to/bf16_torch_dist # BF16 training
   --attention-softmax-in-fp32         # Numerical stability
   --accumulate-allreduce-grads-in-fp32
   # LM head log-probsëŠ” Megatronì´ ìë™ìœ¼ë¡œ FP32ë¡œ upcast
)

MISC_ARGS=(
   --colocate                          # Synchronous mode for reasoning
   --sglang-mem-fraction-static 0.8
   --attention-backend flash
)
```

#### Agentic RL (SWE-bench, Function Calling)

```bash
AGENTIC_ARGS=(
   --custom-generate-function-path module.path:agent_generate
   --custom-rm-path module.path:environment_reward
   --rollout-num-gpus 4                # Disaggregated mode
)

# custom_generate_functionì—ì„œ:
# - loss_mask ì„¤ì • (model tokens = 1, env feedback = 0)
# - Multi-turn interaction loop
# - Environment feedback í†µí•©
```

### 7.3 í•µì‹¬ ì°¨ì´ì  ë° ëˆ„ë½ ê¸°ëŠ¥

#### âŒ Dynamic Temperature (êµ¬í˜„ í•„ìš”)

**GLM-4.5 ë°©ë²•**:
1. Reward plateau ê°ì§€ (variance threshold)
2. Temperature ì¡°ì • (performance drop < 1%)

**slime êµ¬í˜„ ì œì•ˆ**:
```python
# slime/rollout/temperature_scheduler.py (ìƒˆë¡œ ì¶”ê°€)
class DynamicTemperatureScheduler:
    def should_increase_temperature(self, reward_history, window=10):
        recent_std = np.std(reward_history[-window:])
        return recent_std < self.plateau_threshold

    def find_max_temperature(self, current_temp, max_drop=0.01):
        # Binary search or grid search
        pass
```

**Arguments**:
```python
parser.add_argument("--enable-dynamic-temperature", action="store_true")
parser.add_argument("--temperature-plateau-threshold", type=float, default=0.05)
parser.add_argument("--temperature-max-performance-drop", type=float, default=0.01)
```

#### âš ï¸ Token-weighted Loss (í™•ì¸ í•„ìš”)

**GLM-4.5 ë°©ë²•**: í† í° ì¤‘ìš”ë„ ê¸°ë°˜ ê°€ì¤‘ í‰ê· 

**slime í˜„ì¬**: Sequence-mean loss (veRL ê¸°ë³¸ê°’)

**í™•ì¸ ë°©ë²•**:
```python
# slime/backends/megatron_utils/loss.py
# loss aggregation ë°©ë²• í™•ì¸
```

---

## 8. ê¶Œì¥ì‚¬í•­

### 8.1 ì¦‰ì‹œ ì ìš© ê°€ëŠ¥

#### 1. Mean-Centering for Binary Rewards âœ…

```bash
--advantage-estimator grpo
--disable-grpo-std-normalization
```

**ê·¼ê±°**:
- GLM-4.5 ëª…ì‹œì  ì‚¬ìš©
- `BINARY_REWARD_ANALYSIS.md` ì´ë¡ ì  ê²€ì¦
- ì•ˆì •ì ì´ê³  íš¨ìœ¨ì 

#### 2. FP8 Inference for Efficiency âœ…

```bash
--hf-checkpoint /path/to/model-FP8
--attention-softmax-in-fp32           # Numerical stability
--accumulate-allreduce-grads-in-fp32
# LM head log-probsëŠ” Megatronì´ ìë™ìœ¼ë¡œ FP32ë¡œ upcast
```

**íš¨ê³¼**:
- Rollout ì†ë„ í–¥ìƒ
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
- BF16 training ì •ë°€ë„ ìœ ì§€

#### 3. No KL Term in GRPO âœ…

```bash
--kl-coef 0.00
--kl-loss-coef 0.00
```

**ê·¼ê±°**: GLM-4.5 explicit exclusion

#### 4. Outcome Supervision for Agentic Tasks âœ…

```python
# custom_generate_function
def set_loss_mask(sample):
    sample.loss_mask = [
        1 if token_is_model_generated(t) else 0
        for t in sample.tokens
    ]
```

### 8.2 í–¥í›„ êµ¬í˜„ ê¶Œì¥

#### 1. Dynamic Temperature Scheduling

**ìš°ì„ ìˆœìœ„**: High (GLM-4.5 í•µì‹¬ ê¸°ëŠ¥)

**êµ¬í˜„ ë‚œì´ë„**: Medium

**ì˜ˆìƒ íš¨ê³¼**:
- Reward plateauì—ì„œ exploration ìœ ì§€
- í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ

#### 2. Token-weighted Loss for Code RL

**ìš°ì„ ìˆœìœ„**: Medium

**êµ¬í˜„ ë‚œì´ë„**: Low

**ì˜ˆìƒ íš¨ê³¼**:
- ì¤‘ìš” í† í° ì§‘ì¤‘ í•™ìŠµ
- Code generation í’ˆì§ˆ í–¥ìƒ

#### 3. Difficulty-based Curriculum

**ìš°ì„ ìˆœìœ„**: Medium

**êµ¬í˜„ ë‚œì´ë„**: Medium

**êµ¬í˜„**:
```bash
# Stage 1: Moderate difficulty
--prompt-data moderate_problems.parquet
--num-rollout 50

# Stage 2: Extreme difficulty
--prompt-data hard_problems.parquet
--num-rollout 50
```

---

## 9. ì£¼ìš” ë°œê²¬ ë° Insights

### 9.1 Mean-Centering ê²€ì¦

**GLM-4.5 ìˆ˜ì‹**:

$$
\mathcal{L}_{RL}(\theta) = \mathbb{E}\left[\frac{1}{K}\sum (r_i - \bar{r})\right]
$$

**Binary Reward Analysis ê²°ê³¼**:
- Z-Score: ê·¹ë‹¨ ì„±ê³µë¥ ì—ì„œ 2.3-3x gradient ì¦í­
- Mean-Centering: ì•ˆì •ì , ìì—°ìŠ¤ëŸ¬ìš´ ë‚œì´ë„ ê°€ì¤‘

**ê²°ë¡ **: GLM-4.5ì˜ ì„ íƒì€ **ì´ë¡ ì ìœ¼ë¡œ ìµœì **

### 9.2 slimeì˜ Production Readiness

**GLM-4.5ê°€ slimeì„ ì„ íƒí•œ ì´ìœ **:
1. âœ… Colocated + Disaggregated modes
2. âœ… FP8 inference support
3. âœ… Custom generation function (outcome supervision)
4. âœ… Flexible reward model integration
5. âœ… Docker-based agent runtime

**slimeì˜ ê°•ì **:
- **Production-proven**: 355B íŒŒë¼ë¯¸í„° ëª¨ë¸ í•™ìŠµ
- **Flexible**: Synchronous + Asynchronous RL
- **Efficient**: FP8 quantization ì§€ì›

### 9.3 Agentic RLì˜ í•µì‹¬

**Outcome Supervision**:
- Environment feedback ë¬´ì‹œ
- Model-generated tokensë§Œ í•™ìŠµ

**íš¨ê³¼**:
- ì˜ëª»ëœ í™˜ê²½ ì‹ í˜¸ì—ì„œ í•™ìŠµ ë°©ì§€
- ëª¨ë¸ì˜ decision-making ëŠ¥ë ¥ ì§‘ì¤‘ í–¥ìƒ

**slime êµ¬í˜„**: `loss_mask` ë©”ì»¤ë‹ˆì¦˜ ì™„ë²½íˆ ì§€ì›

### 9.4 Deep & Narrow Architecture

**GLM-4.5 ë°œê²¬**:
> "Deeper models exhibited better reasoning capacity"

**ì„¤ê³„ íŠ¸ë ˆì´ë“œì˜¤í”„**:
- **Wider (fewer layers)**: ë³‘ë ¬í™” íš¨ìœ¨ì , ì¶”ë¡  ì†ë„ ë¹ ë¦„
- **Deeper (more layers)**: Reasoning ëŠ¥ë ¥ í–¥ìƒ, sequential processing

**ì„ íƒ**: GLM-4.5ëŠ” **reasoning ìš°ì„ ** (92 layers)

---

## 10. ê²°ë¡ 

### 10.1 êµ¬í˜„ ìƒíƒœ

âœ… **slimeì€ GLM-4.5ì˜ í•µì‹¬ ê¸°ëŠ¥ ëŒ€ë¶€ë¶„ ì§€ì›**
âœ… **Mean-centering (Dr. GRPO) ì´ë¯¸ êµ¬í˜„ë¨**
âœ… **FP8 inference ì§€ì›**
âœ… **Outcome supervision (loss_mask) ì§€ì›**
âœ… **Colocated/Disaggregated modes ì™„ë¹„**

### 10.2 ê°œì„  ê°€ëŠ¥ ì‚¬í•­

ğŸ”§ **Dynamic temperature scheduling** (í•µì‹¬ ëˆ„ë½ ê¸°ëŠ¥)
ğŸ”§ **Token-weighted loss** (Code RL ìµœì í™”)
ğŸ”§ **Curriculum learning utilities** (ë‚œì´ë„ ë‹¨ê³„í™”)

### 10.3 GLM-4.5ì˜ í•µì‹¬ êµí›ˆ

1. **Mean-centering is sufficient**: Binary rewardì— Z-Score ë¶ˆí•„ìš”
2. **Outcome supervision is critical**: Agentic RL ì„±ê³µì˜ í•µì‹¬
3. **Deeper architectures help reasoning**: 92 layers â†’ better CoT
4. **Single-stage long RL works**: 64K output lengthë¥¼ ì ì§„ì  í™•ì¥ ì—†ì´ ì§ì ‘ í•™ìŠµ

### 10.4 slime ì‚¬ìš©ìë¥¼ ìœ„í•œ ê¶Œì¥ì‚¬í•­

**Reasoning/Math Tasks (GSM8K, MATH)**:
```bash
--advantage-estimator grpo
--disable-grpo-std-normalization
--kl-coef 0.00
--hf-checkpoint /path/to/model-FP8
--attention-softmax-in-fp32
--accumulate-allreduce-grads-in-fp32
--colocate
```

**Agentic Tasks (SWE-bench, Function Calling)**:
```bash
--advantage-estimator grpo
--disable-grpo-std-normalization
--custom-generate-function-path path:agent_func
--custom-rm-path path:env_reward
# Disaggregated mode
```

### 10.5 ë‹¤ìŒ ë‹¨ê³„

1. **Dynamic temperature êµ¬í˜„**: `slime/rollout/temperature_scheduler.py`
2. **Token-weighted loss ê²€ì¦**: í˜„ì¬ êµ¬í˜„ í™•ì¸ ë° í•„ìš”ì‹œ ì¶”ê°€
3. **GLM-4.5 ì¬í˜„ ì‹¤í—˜**: ìœ„ ì„¤ì •ìœ¼ë¡œ GSM8K/MATH í…ŒìŠ¤íŠ¸
4. **Benchmark ë¹„êµ**: slime GRPO vs GLM-4.5 reported results

---

## ì°¸ê³  ìë£Œ

- **GLM-4.5 Paper**: https://arxiv.org/html/2508.06471
- **slime Framework**: https://github.com/THUDM/slime
- **Binary Reward Analysis**: `analysis/BINARY_REWARD_ANALYSIS.md`
- **CISPO Review**: `analysis/CISPO_PAPER_REVIEW.md`
- **Dr. GRPO Paper**: https://arxiv.org/pdf/2503.20783

---

**ë¬¸ì„œ ì‘ì„±ì¼**: 2025-11-01
**ë¦¬ë·°ì–´**: Claude Code
**GLM-4.5 Paper Version**: arXiv:2508.06471
**slime ë¸Œëœì¹˜**: dev
